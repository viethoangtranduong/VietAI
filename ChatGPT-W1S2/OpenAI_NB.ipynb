{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "Ygvrhqf7hZXi",
        "gOaOI7ueYA6I",
        "b3kBhgT2cwme",
        "9JHoIfUlecDA"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "elWVfAI_XEz8",
        "outputId": "2b756525-f386-457a-8b6d-f15bf7c05ac9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-0.28.0-py3-none-any.whl (76 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/76.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m71.7/76.5 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.5/76.5 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai) (3.8.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2023.7.22)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.1)\n",
            "Installing collected packages: openai\n",
            "Successfully installed openai-0.28.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Bs_z5661VOtX"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "openai.api_key = \"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "p-HFo8mbXKWH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ChatCompletion"
      ],
      "metadata": {
        "id": "_iuZwx0_XLoM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parameters"
      ],
      "metadata": {
        "id": "Ygvrhqf7hZXi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Required parameters"
      ],
      "metadata": {
        "id": "QmjpMAo_X5xV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = openai.ChatCompletion.create(model = \"gpt-3.5-turbo\",\n",
        "                                      messages = [dict(role=\"user\", content = \"Who are you?\")])\n",
        "\n",
        "result.choices[0].message.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "7GWMoNYKXKan",
        "outputId": "d52f7ade-a85d-481d-861a-f5443f0d2218"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I am an AI developed by OpenAI, designed to assist with various tasks and provide information on a wide range of topics.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### temperature\n",
        "What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic."
      ],
      "metadata": {
        "id": "ZPiF72wFX7qL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(3):\n",
        "  result = openai.ChatCompletion.create(model = \"gpt-3.5-turbo\",\n",
        "                                        messages = [dict(role=\"user\", content = \"Who are you?\"),],\n",
        "                                        temperature = 0)\n",
        "\n",
        "  print(result.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y0-gQFc0WJIG",
        "outputId": "61140b50-7d5d-4715-9898-97554fbeeee2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I am an AI language model developed by OpenAI. I am designed to assist with answering questions and engaging in conversations on a wide range of topics.\n",
            "I am an AI language model developed by OpenAI. I am designed to assist with answering questions and engaging in conversations on a wide range of topics.\n",
            "I am an AI language model developed by OpenAI. I am designed to assist with answering questions and engaging in conversations on a wide range of topics.\n",
            "I am an AI language model developed by OpenAI. I am designed to assist with answering questions and engaging in conversations on a wide range of topics.\n",
            "I am an AI language model developed by OpenAI. I am designed to assist with answering questions and engaging in conversations on a wide range of topics.\n",
            "I am an AI language model developed by OpenAI. I am designed to assist with answering questions and engaging in conversations on a wide range of topics.\n",
            "I am an AI language model developed by OpenAI. I am designed to assist with answering questions and engaging in conversations on a wide range of topics.\n",
            "I am an AI language model developed by OpenAI. I am designed to assist with answering questions and engaging in conversations on a wide range of topics.\n",
            "I am an AI language model developed by OpenAI. I am designed to assist with answering questions and engaging in conversations on a wide range of topics.\n",
            "I am an AI language model developed by OpenAI. I am designed to assist with answering questions and engaging in conversations on a wide range of topics.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### top_p\n",
        "An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered."
      ],
      "metadata": {
        "id": "gOaOI7ueYA6I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(3):\n",
        "  result = openai.ChatCompletion.create(model = \"gpt-3.5-turbo\",\n",
        "                                        messages = [dict(role=\"user\", content = \"Who are you?\"),],\n",
        "                                        top_p = 0)\n",
        "\n",
        "  print(result.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VzUPdnv-WJKw",
        "outputId": "b633644a-7723-4b7c-9d16-45c0db349a2e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I am an AI language model developed by OpenAI. I am designed to assist with answering questions and engaging in conversations on a wide range of topics.\n",
            "I am an AI language model developed by OpenAI. I am designed to assist with answering questions and engaging in conversations on a wide range of topics.\n",
            "I am an AI language model developed by OpenAI. I am designed to assist with answering questions and engaging in conversations on a wide range of topics.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### n\n",
        "How many chat completion choices to generate for each input message."
      ],
      "metadata": {
        "id": "4r-asrZHYtvf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = openai.ChatCompletion.create(model = \"gpt-3.5-turbo\",\n",
        "                                        messages = [dict(role=\"user\", content = \"Who are you?\"),],\n",
        "                                        n = 3)"
      ],
      "metadata": {
        "id": "5vdWdJtdWJNJ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(3):\n",
        "  print(result.choices[i].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7dwhfXUJWJQV",
        "outputId": "c2c49ea0-1fe4-4db7-aeae-40f9cca819ff"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I am an AI developed by OpenAI, known as GPT-3. I am an advanced language model designed to understand and generate human-like text based on the information provided to me.\n",
            "I am a virtual assistant developed by OpenAI. I am here to help answer your questions and assist with tasks. Is there anything specific you would like assistance with?\n",
            "I am an AI language model developed by OpenAI.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### stream\n",
        "If set, partial message deltas will be sent, like in ChatGPT. Tokens will be sent as data-only server-sent events as they become available, with the stream terminated by a data: [DONE] message."
      ],
      "metadata": {
        "id": "RUowPSvfZFVw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = openai.ChatCompletion.create(model = \"gpt-3.5-turbo\",\n",
        "                                        messages = [dict(role=\"user\", content = \"Who are you?\"),],\n",
        "                                        stream = True)"
      ],
      "metadata": {
        "id": "DgDl28bdWJST"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for val in result:\n",
        "  print(val.choices[0])\n",
        "  print(\"###\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cd1MoXpWWJU6",
        "outputId": "635f1f41-79cf-4fcc-842c-39be6fad2aeb"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"index\": 0,\n",
            "  \"delta\": {\n",
            "    \"role\": \"assistant\",\n",
            "    \"content\": \"\"\n",
            "  },\n",
            "  \"finish_reason\": null\n",
            "}\n",
            "###\n",
            "{\n",
            "  \"index\": 0,\n",
            "  \"delta\": {\n",
            "    \"content\": \"I\"\n",
            "  },\n",
            "  \"finish_reason\": null\n",
            "}\n",
            "###\n",
            "{\n",
            "  \"index\": 0,\n",
            "  \"delta\": {\n",
            "    \"content\": \" am\"\n",
            "  },\n",
            "  \"finish_reason\": null\n",
            "}\n",
            "###\n",
            "{\n",
            "  \"index\": 0,\n",
            "  \"delta\": {\n",
            "    \"content\": \" an\"\n",
            "  },\n",
            "  \"finish_reason\": null\n",
            "}\n",
            "###\n",
            "{\n",
            "  \"index\": 0,\n",
            "  \"delta\": {\n",
            "    \"content\": \" AI\"\n",
            "  },\n",
            "  \"finish_reason\": null\n",
            "}\n",
            "###\n",
            "{\n",
            "  \"index\": 0,\n",
            "  \"delta\": {\n",
            "    \"content\": \" language\"\n",
            "  },\n",
            "  \"finish_reason\": null\n",
            "}\n",
            "###\n",
            "{\n",
            "  \"index\": 0,\n",
            "  \"delta\": {\n",
            "    \"content\": \" model\"\n",
            "  },\n",
            "  \"finish_reason\": null\n",
            "}\n",
            "###\n",
            "{\n",
            "  \"index\": 0,\n",
            "  \"delta\": {\n",
            "    \"content\": \" developed\"\n",
            "  },\n",
            "  \"finish_reason\": null\n",
            "}\n",
            "###\n",
            "{\n",
            "  \"index\": 0,\n",
            "  \"delta\": {\n",
            "    \"content\": \" by\"\n",
            "  },\n",
            "  \"finish_reason\": null\n",
            "}\n",
            "###\n",
            "{\n",
            "  \"index\": 0,\n",
            "  \"delta\": {\n",
            "    \"content\": \" Open\"\n",
            "  },\n",
            "  \"finish_reason\": null\n",
            "}\n",
            "###\n",
            "{\n",
            "  \"index\": 0,\n",
            "  \"delta\": {\n",
            "    \"content\": \"AI\"\n",
            "  },\n",
            "  \"finish_reason\": null\n",
            "}\n",
            "###\n",
            "{\n",
            "  \"index\": 0,\n",
            "  \"delta\": {\n",
            "    \"content\": \".\"\n",
            "  },\n",
            "  \"finish_reason\": null\n",
            "}\n",
            "###\n",
            "{\n",
            "  \"index\": 0,\n",
            "  \"delta\": {\n",
            "    \"content\": \" My\"\n",
            "  },\n",
            "  \"finish_reason\": null\n",
            "}\n",
            "###\n",
            "{\n",
            "  \"index\": 0,\n",
            "  \"delta\": {\n",
            "    \"content\": \" purpose\"\n",
            "  },\n",
            "  \"finish_reason\": null\n",
            "}\n",
            "###\n",
            "{\n",
            "  \"index\": 0,\n",
            "  \"delta\": {\n",
            "    \"content\": \" is\"\n",
            "  },\n",
            "  \"finish_reason\": null\n",
            "}\n",
            "###\n",
            "{\n",
            "  \"index\": 0,\n",
            "  \"delta\": {\n",
            "    \"content\": \" to\"\n",
            "  },\n",
            "  \"finish_reason\": null\n",
            "}\n",
            "###\n",
            "{\n",
            "  \"index\": 0,\n",
            "  \"delta\": {\n",
            "    \"content\": \" provide\"\n",
            "  },\n",
            "  \"finish_reason\": null\n",
            "}\n",
            "###\n",
            "{\n",
            "  \"index\": 0,\n",
            "  \"delta\": {\n",
            "    \"content\": \" information\"\n",
            "  },\n",
            "  \"finish_reason\": null\n",
            "}\n",
            "###\n",
            "{\n",
            "  \"index\": 0,\n",
            "  \"delta\": {\n",
            "    \"content\": \" and\"\n",
            "  },\n",
            "  \"finish_reason\": null\n",
            "}\n",
            "###\n",
            "{\n",
            "  \"index\": 0,\n",
            "  \"delta\": {\n",
            "    \"content\": \" assist\"\n",
            "  },\n",
            "  \"finish_reason\": null\n",
            "}\n",
            "###\n",
            "{\n",
            "  \"index\": 0,\n",
            "  \"delta\": {\n",
            "    \"content\": \" with\"\n",
            "  },\n",
            "  \"finish_reason\": null\n",
            "}\n",
            "###\n",
            "{\n",
            "  \"index\": 0,\n",
            "  \"delta\": {\n",
            "    \"content\": \" various\"\n",
            "  },\n",
            "  \"finish_reason\": null\n",
            "}\n",
            "###\n",
            "{\n",
            "  \"index\": 0,\n",
            "  \"delta\": {\n",
            "    \"content\": \" tasks\"\n",
            "  },\n",
            "  \"finish_reason\": null\n",
            "}\n",
            "###\n",
            "{\n",
            "  \"index\": 0,\n",
            "  \"delta\": {\n",
            "    \"content\": \".\"\n",
            "  },\n",
            "  \"finish_reason\": null\n",
            "}\n",
            "###\n",
            "{\n",
            "  \"index\": 0,\n",
            "  \"delta\": {},\n",
            "  \"finish_reason\": \"stop\"\n",
            "}\n",
            "###\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### stop\n",
        "Up to 4 sequences where the API will stop generating further tokens."
      ],
      "metadata": {
        "id": "efEtsmJJZvV1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = openai.ChatCompletion.create(model = \"gpt-3.5-turbo\",\n",
        "                                      messages = [dict(role=\"user\", content = \"Who are you?\"),],\n",
        "                                      temperature = 0,\n",
        "                                      stop = [\"OpenAI\"])\n",
        "result.choices[0].message.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "BzVnMzJHWJZ1",
        "outputId": "d7e2130e-0a6e-40b5-f749-1a8da0152ceb"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I am an AI language model developed by '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### max_tokens\n",
        "The maximum number of tokens to generate in the chat completion.\n",
        "\n",
        "The total length of input tokens and generated tokens is limited by the model's context length."
      ],
      "metadata": {
        "id": "N1nQlJYsZ7hC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = openai.ChatCompletion.create(model = \"gpt-3.5-turbo\",\n",
        "                                      messages = [dict(role=\"user\", content = \"Who are you?\"),],\n",
        "                                      max_tokens = 10,)\n",
        "result.choices[0].message.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "07IAnKNKWJd6",
        "outputId": "60b49e1b-a9e2-4cd6-8fa1-de17279823a9"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I am an AI language model developed by OpenAI'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### presence_penalty\n",
        "Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics."
      ],
      "metadata": {
        "id": "08q9Al1uaOBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = openai.ChatCompletion.create(model = \"gpt-3.5-turbo\",\n",
        "                                      messages = [dict(role=\"user\", content = \"Who are you?\"),],\n",
        "                                      presence_penalty = 2)\n",
        "result.choices[0].message.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "tAFSF43ZZ68S",
        "outputId": "9e8b52a9-fe6b-4147-bea9-581a465ab7d7"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"I am an artificial intelligence created by OpenAI. I don't have a physical form and exist solely as a language model designed to assist with answering questions and providing information.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### frequency_penalty\n",
        "Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim."
      ],
      "metadata": {
        "id": "wM6Wnl8RabSa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = openai.ChatCompletion.create(model = \"gpt-3.5-turbo\",\n",
        "                                      messages = [dict(role=\"user\", content = \"Who are you?\"),],\n",
        "                                      frequency_penalty = 2)\n",
        "result.choices[0].message.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "SXfVEEH6WJfW",
        "outputId": "797e0bbd-f95f-49ff-b7ba-e23ac0808879"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I am an AI language model developed by OpenAI. I am designed to provide information and engage in conversation on a wide range of topics.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### logit_bias\n",
        "Modify the likelihood of specified tokens appearing in the completion.\n",
        "\n",
        "Accepts a json object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token."
      ],
      "metadata": {
        "id": "WYCfwt1haoqb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken\n",
        "import tiktoken\n",
        "\n",
        "encoding_model = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
        "encoding_model.encode(\"banana\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vXgtom-Pbza_",
        "outputId": "fc1d1482-cf23-41cd-8209-17a7fa71bd7e"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (0.4.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.6.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2023.7.22)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[88847]"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = openai.ChatCompletion.create(model = \"gpt-3.5-turbo\",\n",
        "                                      messages = [dict(role=\"user\", content = \"Who are you?\"),],\n",
        "                                      logit_bias = {88847:40},\n",
        "                                      max_tokens=10)\n",
        "result.choices[0].message.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "NhLU9k1GWJh6",
        "outputId": "1dbdbcd2-0a5d-48c3-ea6b-e8325d783544"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'bananabananabananabananabananabananabananabananabananabanana'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "A5ANLm2obPvZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prompts (refresh)"
      ],
      "metadata": {
        "id": "b3kBhgT2cwme"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ZSL (pure prompt)"
      ],
      "metadata": {
        "id": "OwV9w9iweGe_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Extract name from 'this is VietAI course'\"\n",
        "\n",
        "result = openai.ChatCompletion.create(model = \"gpt-3.5-turbo\",\n",
        "                                      messages = [dict(role=\"user\", content = prompt),],\n",
        "                                      temperature=0)\n",
        "result.choices[0].message.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "AYFj5HXmczem",
        "outputId": "99b149a9-3224-492f-e60e-f311d49f5a52"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"There is no name mentioned in the given phrase 'this is VietAI course'.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Few Shot Learning (show examples)"
      ],
      "metadata": {
        "id": "c-gZJnfteKK-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"Examples:\n",
        "Extract name from 'this is Banana course' -> Banana\n",
        "Extract name from 'this is VinFast cars' -> VinFast\n",
        "Extract name from 'this is VietAI course' -> \"\"\"\n",
        "\n",
        "result = openai.ChatCompletion.create(model = \"gpt-3.5-turbo\",\n",
        "                                      messages = [dict(role=\"user\", content = prompt),],\n",
        "                                      temperature=0)\n",
        "result.choices[0].message.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "r2Cay9BhdEx1",
        "outputId": "b6bf52b8-e331-44ac-fca1-361fb6b46f4a"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'VietAI'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chain of Thought (show steps)"
      ],
      "metadata": {
        "id": "oa39IIvFeO1I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"Extract name from 'this is VietAI course'\n",
        "Step 1: Extract all the nouns\n",
        "Step 2: Select the most representative noun\n",
        "Output: \"\"\"\n",
        "\n",
        "result = openai.ChatCompletion.create(model = \"gpt-3.5-turbo\",\n",
        "                                      messages = [dict(role=\"user\", content = prompt),],\n",
        "                                      temperature=0)\n",
        "result.choices[0].message.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "GyobmcSAdXu7",
        "outputId": "c4037ada-12dd-4875-e457-f303e0dec546"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'VietAI'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Specify Input/Output format (constrain output)"
      ],
      "metadata": {
        "id": "LNvV41nOeVrJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"Extract name from 'this is VietAI course'\n",
        "Step 1: Extract all the nouns\n",
        "Step 2: Select the most representative noun\n",
        "Output a json with key \"label\", value is the step 2 output. Json \"\"\"\n",
        "\n",
        "result = openai.ChatCompletion.create(model = \"gpt-3.5-turbo\",\n",
        "                                      messages = [dict(role=\"user\", content = prompt),],\n",
        "                                      temperature=0)\n",
        "result.choices[0].message.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "qdEdhG9ydyAp",
        "outputId": "7da26dfb-0bde-4634-b990-218b549ee00d"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'{\\n  \"label\": \"VietAI\"\\n}'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iq0vOLh0d6th"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tenacity / Retry mechanism for stability\n",
        "OpenAI APIs when calling in large batch can hit RateLimit/Server Errors -> use tenacity to retry to avoid raising errors and blocking your jobs"
      ],
      "metadata": {
        "id": "9JHoIfUlecDA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tenacity import (\n",
        "    retry,\n",
        "    stop_after_attempt,\n",
        "    wait_exponential,\n",
        "    wait_fixed,\n",
        ")  # for exponential backoff\n",
        "import random\n",
        "\n",
        "@retry(wait=wait_exponential(multiplier=2, min=1, max=60), stop=stop_after_attempt(6))\n",
        "def openai_completion_with_backoff(**kwargs):\n",
        "    return openai.ChatCompletion.create(**kwargs)\n",
        "\n",
        "## Test Func only, do not use this\n",
        "@retry(wait=wait_fixed(3), stop=stop_after_attempt(6))\n",
        "def openai_completion_with_backoff(**kwargs):\n",
        "    random_error = random.random()\n",
        "    if random_error < 0.7:\n",
        "      print(\"Random error\", random_error)\n",
        "      raise ValueError(\"Random error\")\n",
        "    else:\n",
        "      print(\"No random error\", random_error)\n",
        "      return openai.ChatCompletion.create(**kwargs)\n",
        "\n",
        "\n",
        "params = dict(model=\"gpt-3.5-turbo\", messages=[dict(role=\"user\", content = \"Who are you?\")])\n",
        "result = openai_completion_with_backoff(**params)\n",
        "result.choices[0].message.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "7CwL4OEyenkx",
        "outputId": "8aceaab4-b106-4d13-90e4-27e7a31f3e58"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random error 0.03601480870318963\n",
            "Random error 0.38351870767363005\n",
            "No random error 0.9879954315112975\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I am an AI language model developed by OpenAI. I am programmed to provide information and engage in conversations on a wide range of topics. How can I assist you today?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qSvgl1JZfNSz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "C27lCPtmhQpK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embeddings"
      ],
      "metadata": {
        "id": "osRMPFkHhRRF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = openai.Embedding.create(\n",
        "    input=\"This is the embeddings of a text\",\n",
        "    model=\"text-embedding-ada-002\"\n",
        ")\n",
        "embeddings = response['data'][0]['embedding']"
      ],
      "metadata": {
        "id": "EYQq1EhihQr9"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(embeddings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XjrzciMyhQug",
        "outputId": "8a309842-2537-4c07-d06a-c656092d85f9"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1536"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xFDogX6lhtfa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://huggingface.co/spaces/mteb/leaderboard"
      ],
      "metadata": {
        "id": "lQKXay_eiFQh"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Z3QPluBikpRH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Speech"
      ],
      "metadata": {
        "id": "ge0Y6ifFktvs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Note: you need to be using OpenAI Python v0.27.0 for the code below to work\n",
        "audio_file= open(\"One-Call-Away-Charlie-Puth.mp3\", \"rb\")\n",
        "transcript = openai.Audio.transcribe(\"whisper-1\", audio_file)"
      ],
      "metadata": {
        "id": "akJrzrJFkxIv"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transcript"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0lv54i1ypGiM",
        "outputId": "7ec91aeb-c772-4cf0-d588-7f3cbd9cb0f0"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<OpenAIObject at 0x7af5e9d5d260> JSON: {\n",
              "  \"text\": \"I'm only one call away. I'll be there to save the day. Superman got nothing on me. I'm only one call away. Call me baby if you need a friend. I just wanna give you love. Come on, come on, come on. Reaching out to you so take a chance. No matter where you go, you know you're not alone. I'm only one call away. I'll be there to save the day. Superman got nothing on me. I'm only one call away. Come along with me and don't be scared. I just wanna set you free. Come on, come on, come on. You and me can make it anywhere. But for now, we can stay here for a while. Cause you know I just wanna see you smile. No matter where you go, you know you're not alone. I'm only one call away. I'll be there to save the day. Superman got nothing on me. I'm only one call away. And when you're weak, I'll be strong. I'm gonna keep holding on. Now don't you worry, it won't be long, darling. When you feel like hope is gone, just run into my arms. I'm only one call away. I'll be there to save the day. Superman got nothing on me. I'm only one, I'm only one call away. I'll be there to save the day. Superman got nothing on me. I'm only one call away. I'm only one call away. I'll be there to save the day. Superman got nothing on me. I'm only one, I'm only one call away.\"\n",
              "}"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "# import high-level pipeline\n",
        "from transformers import pipeline\n",
        "\n",
        "# This line will load your desired model + GPU activated\n",
        "pipe = pipeline(\"automatic-speech-recognition\", model=\"openai/whisper-small\")\n",
        "\n",
        "# this will handle the longer audio file with faster inference utilizing GPU\n",
        "print(pipe(\"One-Call-Away-Charlie-Puth.mp3\", max_new_tokens=100, chunk_length_s=10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YWe5cGfclLUr",
        "outputId": "67e1623c-8cfc-4505-f173-f0b827029ab4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'text': \" I'm only one call away I'll be there to save the day Superman The day Superman got nothing on me I'm only one call away call me baby if you need a friend I just wanna give you love Come on, come on, come on Reaching out to you, so take a chance I'm only one Call away I'll be there to save the day Superman got nothing to make Then I'm mad, I'm only one cause of blame Come along with me and don't be scared I just want to set you free Come on, come on, come on You and me can make it anywhere If and now we could stay here for a while Cause you know I just wanna see you smile You know you're not alone I'm only one Call the way I'll be there to save the day I've got a safe thought of death Superman got nothing on me Then I'm mad, I'm only one call away And when you're weak, I'll be strong I'm gonna keep holding on Now don't you worry, it won't be long, darling When you say like hope is gone Just run into my arms, I'm only one Call away, I'll be there to save the day Superman got nothing I'm only one, I'm on my way, I'll be there to save the day Superman got nothing on that plane I'm only one call away I'm only one call away Call the way you\"}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "X2RjT7oImEtB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Image"
      ],
      "metadata": {
        "id": "DPxlHaitrc6D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = openai.Image.create(\n",
        "  prompt=\"A VietAI logo with green logo and black background, using AI logos\",\n",
        "  n=1,\n",
        "  size=\"1024x1024\"\n",
        ")\n",
        "image_url = response['data'][0]['url']"
      ],
      "metadata": {
        "id": "6AZBR1rhrdyq"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_url"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "47imuJ0XrlDp",
        "outputId": "432e18fc-4f80-4856-c4ea-067dfa03aaac"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'https://oaidalleapiprodscus.blob.core.windows.net/private/org-qSmfpzIuUUuB1e7LWCSBwCc8/user-14bNgsusNknHppTRYDEMtUJZ/img-X9udTQYHCzNAq8bX4r8S0Byt.png?st=2023-09-10T14%3A20%3A54Z&se=2023-09-10T16%3A20%3A54Z&sp=r&sv=2021-08-06&sr=b&rscd=inline&rsct=image/png&skoid=6aaadede-4fb3-4698-a8f6-684d7786b067&sktid=a48cca56-e6da-484e-a814-9c849652bcb3&skt=2023-09-09T18%3A24%3A19Z&ske=2023-09-10T18%3A24%3A19Z&sks=b&skv=2021-08-06&sig=6EY3IkcqxosEqfoBDG4uUtr0UkfM7ibWHjgMkYlDlsU%3D'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FK5jaUvDrrS-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}